{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Benchmark Orchestrator - optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import json\n",
    "\n",
    "from pyrit.common import IN_MEMORY, initialize_pyrit\n",
    "from pyrit.datasets import fetch_wmdp_dataset\n",
    "from pyrit.memory import CentralMemory\n",
    "from pyrit.models import (\n",
    "    PromptRequestResponse,\n",
    "    QuestionAnsweringDataset,\n",
    "    QuestionAnsweringEntry,\n",
    "    QuestionChoice,\n",
    ")\n",
    "from pyrit.orchestrator import QuestionAnsweringBenchmarkOrchestrator\n",
    "from pyrit.prompt_target import OpenAIChatTarget\n",
    "from pyrit.score.question_answer_scorer import QuestionAnswerScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PyRIT (load environment files and set central memory instance)\n",
    "initialize_pyrit(memory_db_type=IN_MEMORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Set up the Azure OpenAI prompt target\n",
    "target = OpenAIChatTarget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function for scoring\n",
    "def score_responses(responses: list[PromptRequestResponse]):\n",
    "    correct_count = 0\n",
    "    for response in responses:\n",
    "        score_metadata = json.loads(response.request_pieces[0].scores[0].score_metadata)\n",
    "        correct_answer = score_metadata[\"correct_answer\"]\n",
    "        received_answer = score_metadata[\"scored_answer\"]\n",
    "        print(f\"Was answer correct: {response.request_pieces[0].scores[0].score_value}\")\n",
    "        print(f\"Correct Answer: {correct_answer}\")\n",
    "        print(f\"Answer Received: {received_answer}\")\n",
    "        correct_count += int(response.request_pieces[0].scores[0].score_value == \"True\")\n",
    "    print(f\"Correct / Total: {correct_count} / {len(responses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create demo dataset for Q/A Model\n",
    "qa_ds = QuestionAnsweringDataset(\n",
    "    name=\"demo dataset\",\n",
    "    version=\"1.0\",\n",
    "    description=\"A dataset for testing the question answering model\",\n",
    "    author=\"PyRIT Team\",\n",
    "    group=\"\",\n",
    "    source=\"\",\n",
    "    questions=[\n",
    "        QuestionAnsweringEntry(\n",
    "            question=\"What is the capital of France?\",\n",
    "            answer_type=\"str\",\n",
    "            correct_answer=\"0\",\n",
    "            choices=[\n",
    "                QuestionChoice(index=0, text=\"Paris\"),\n",
    "                QuestionChoice(index=1, text=\"London\"),\n",
    "                QuestionChoice(index=2, text=\"Berlin\"),\n",
    "                QuestionChoice(index=3, text=\"Madrid\"),\n",
    "            ],\n",
    "        ),\n",
    "        QuestionAnsweringEntry(\n",
    "            question=\"What is the capital of Germany?\",\n",
    "            answer_type=\"str\",\n",
    "            correct_answer=\"2\",\n",
    "            choices=[\n",
    "                QuestionChoice(index=0, text=\"Paris\"),\n",
    "                QuestionChoice(index=1, text=\"London\"),\n",
    "                QuestionChoice(index=2, text=\"Berlin\"),\n",
    "                QuestionChoice(index=3, text=\"Madrid\"),\n",
    "            ],\n",
    "        ),\n",
    "        QuestionAnsweringEntry(\n",
    "            question=\"Which of the following is a prime number?\",\n",
    "            answer_type=\"str\",\n",
    "            correct_answer=\"3\",\n",
    "            choices=[\n",
    "                QuestionChoice(index=0, text=\"507963\"),\n",
    "                QuestionChoice(index=1, text=\"507962\"),\n",
    "                QuestionChoice(index=2, text=\"507960\"),\n",
    "                QuestionChoice(index=3, text=\"507961\"),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Create the scorer\n",
    "qa_scorer = QuestionAnswerScorer()\n",
    "\n",
    "# Create the orchestrator with scorer and demo dataset\n",
    "benchmark_orchestrator = QuestionAnsweringBenchmarkOrchestrator(\n",
    "    objective_target=target, scorers=[qa_scorer], verbose=True\n",
    ")\n",
    "\n",
    "# Sends prompts associated with dataset\n",
    "responses = await benchmark_orchestrator.send_prompts_async(dataset=qa_ds)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_responses(responses=responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch WMDP dataset for Q/A Model Testing\n",
    "\n",
    "wmdp_ds = fetch_wmdp_dataset()\n",
    "wmdp_ds.questions = wmdp_ds.questions[:3]\n",
    "\n",
    "# Evaluate the Q/A Model response\n",
    "responses = await benchmark_orchestrator.send_prompts_async(dataset=wmdp_ds)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output if the results are correct\n",
    "score_responses(responses=responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch WMDP dataset for Q/A Model Testing - Chem Subset\n",
    "\n",
    "wmdp_ds = fetch_wmdp_dataset(category=\"chem\")\n",
    "wmdp_ds.questions = wmdp_ds.questions[:3]\n",
    "\n",
    "# Evaluate the Q/A Model response\n",
    "responses = await benchmark_orchestrator.send_prompts_async(dataset=wmdp_ds)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output if the results are correct\n",
    "score_responses(responses=responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch WMDP dataset for Q/A Model Testing - Bio Subset\n",
    "\n",
    "wmdp_ds = fetch_wmdp_dataset(category=\"bio\")\n",
    "wmdp_ds.questions = wmdp_ds.questions[:3]\n",
    "\n",
    "# Evaluate the Q/A Model response\n",
    "responses = await benchmark_orchestrator.send_prompts_async(dataset=wmdp_ds)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output if the results are correct\n",
    "score_responses(responses=responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch WMDP dataset for Q/A Model Testing - Cyber Subset\n",
    "\n",
    "wmdp_ds = fetch_wmdp_dataset(category=\"cyber\")\n",
    "wmdp_ds.questions = wmdp_ds.questions[:3]\n",
    "\n",
    "# Evaluate the Q/A Model response\n",
    "responses = await benchmark_orchestrator.send_prompts_async(dataset=wmdp_ds)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output if the results are correct\n",
    "score_responses(responses=responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close connection for memory instance\n",
    "memory = CentralMemory.get_memory_instance()\n",
    "memory.dispose_engine()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
